{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed398fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "cluster-84ed  GCE       4                                             RUNNING  us-central1-c\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9daec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import hashlib\n",
    "from time import time\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from operator import add\n",
    "\n",
    "from google.cloud import storage\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30410230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/10 15:21:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InvertedIndex\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26944cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "# Set up stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78260f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROJECT_NAME = 'ir-project-415515'\n",
    "BUCKET_NAME = 'irproj_26051997'\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c72c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
    "\n",
    "def word_count(id, tokens):\n",
    "    token_counts = Counter(tokens)\n",
    "    result = [(token, (id, count)) for token, count in token_counts.items()]\n",
    "    return result\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    return postings.map(lambda x: (x[0], len(x[1]))) # (token, df) - df - in how many documents the term ocurred\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, index):\n",
    "    map_to_buckets = postings.map(lambda item: (token2bucket_id(item[0]), item)).groupByKey()\n",
    "    return map_to_buckets.map(lambda x: InvertedIndex.write_a_posting_list(x, index.base_dir, BUCKET_NAME))\n",
    "\n",
    "\n",
    "def create_anchor_list(page):\n",
    "    doc_id, anchors = page[0], page[1]     \n",
    "    return [(doc_id, anchor[1]) for anchor in anchors]\n",
    "\n",
    "\n",
    "def create_index(doc_pairs, directory, filter_tf=False):\n",
    "    inverted = InvertedIndex(base_dir=directory)\n",
    "    \n",
    "    doc_pairs = doc_pairs.map(lambda pair: (pair[0], tokenize(pair[1])))\n",
    "    print(\"Done tokenization\")\n",
    "    \n",
    "    # Calculate word counts and filter\n",
    "    word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    w2df_dict = calculate_df(postings).collectAsMap()\n",
    "    \n",
    "    if filter_tf:\n",
    "        postings = postings.filter(lambda x: len(x[1]) > 50)\n",
    "        \n",
    "    print(\"Done posting lists creation and df creation\")\n",
    "\n",
    "    _ = partition_postings_and_write(postings, inverted).collect()\n",
    "    \n",
    "    # Collect all posting lists locations into one super-set\n",
    "    super_posting_locs = defaultdict(list)\n",
    "    for blob in client.list_blobs(BUCKET_NAME, prefix=directory):\n",
    "        if not blob.name.endswith(\"pickle\"):\n",
    "            continue\n",
    "        with blob.open(\"rb\") as f:\n",
    "            posting_locs = pickle.load(f)\n",
    "            for k, v in posting_locs.items():\n",
    "                super_posting_locs[k].extend(v)\n",
    "    \n",
    "    print(\"Done creating a posting locs list\")\n",
    "    \n",
    "    # Create and configure InvertedIndex instance\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "    print(\"Saved posting locs\")\n",
    "\n",
    "    inverted.df.update(w2df_dict)\n",
    "    print(\"Updated df\")\n",
    "\n",
    "    inverted.term_total.update(postings.flatMapValues(lambda x: x).map(lambda x: (x[0], x[1][1])).reduceByKey(add).collectAsMap())\n",
    "    print(\"Updated tf\")\n",
    "    \n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30561012",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 109\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done posting lists creation and df creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating a posting locs list\n",
      "Saved posting locs\n",
      "Updated df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tf\n",
      "Created title index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n",
      "\\ [1 files][132.9 MiB/132.9 MiB]                                                \n",
      "Operation completed over 1 objects/132.9 MiB.                                    \n",
      "Title index saved to bucket successfully\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "# Set up Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# Get list of blobs in bucket\n",
    "full_path = f\"gs://{BUCKET_NAME}/\"\n",
    "\n",
    "blobs = [b for b in client.list_blobs(BUCKET_NAME, prefix='wiki_files/') if b.name not in ['wiki_files/graphframes.sh', 'wiki_files/']]\n",
    "paths = [full_path + b.name for b in blobs]\n",
    "\n",
    "# Read parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "doc_title_pairs = parquetFile.select(\"id\", \"title\").rdd\n",
    "inverted_title = create_index(doc_title_pairs, f'indices/title_index/postings_title_gcp/')\n",
    "print('Created title index')\n",
    "\n",
    "doc_token_counts_title = doc_title_pairs.map(lambda x: (x[0], len(tokenize(x[1]))))\n",
    "inverted_title.doc_lengths = doc_token_counts_title.collectAsMap()\n",
    "inverted_title.num_docs = len(inverted_title.doc_lengths.keys())\n",
    "inverted_title.avg_doc_length = np.mean(np.array(list(inverted_title.doc_lengths.values())))\n",
    "\n",
    "\n",
    "# Write global stats and upload to Google Storage\n",
    "inverted_title.write_index('.', 'title_index')\n",
    "index_src = \"title_index.pkl\"\n",
    "index_dst = f'gs://{BUCKET_NAME}/indices/title_index/postings_title_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n",
    "print('Title index saved to bucket successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48c0a8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "name = \"title_id_dict\"\n",
    "\n",
    "with open(f\"{name}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(doc_title_pairs.collectAsMap(), f)\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"title_id_dict/{name}.pkl\")\n",
    "blob.upload_from_filename(f\"{name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7d632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/10 15:21:45 WARN TaskSetManager: Lost task 12.0 in stage 2.0 (TID 81) (cluster-84ed-w-3.c.ir-project-415515.internal executor 6): TaskKilled (Stage cancelled)\n",
      "24/03/10 15:21:46 WARN TaskSetManager: Lost task 9.2 in stage 2.0 (TID 80) (cluster-84ed-w-3.c.ir-project-415515.internal executor 8): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
