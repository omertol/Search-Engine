{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed398fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "cluster-84ed  GCE       4                                             RUNNING  us-central1-c\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9daec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import hashlib\n",
    "from time import time\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from operator import add\n",
    "\n",
    "from google.cloud import storage\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30410230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/10 14:33:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InvertedIndex\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26944cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "# Set up stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78260f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROJECT_NAME = 'ir-project-415515'\n",
    "BUCKET_NAME = 'irproj_26051997'\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c72c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
    "\n",
    "def word_count(id, tokens, idx):\n",
    "    token_counts = Counter(tokens)\n",
    "    result = [(token, (id, count)) for token, count in token_counts.items()]\n",
    "    idx.num_docs += 1\n",
    "    return result\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    return postings.map(lambda x: (x[0], len(x[1]))) # (token, df) - df - in how many documents the term ocurred\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, index):\n",
    "    map_to_buckets = postings.map(lambda item: (token2bucket_id(item[0]), item)).groupByKey()\n",
    "    return map_to_buckets.map(lambda x: InvertedIndex.write_a_posting_list(x, index.base_dir, BUCKET_NAME))\n",
    "\n",
    "\n",
    "def create_anchor_list(page):\n",
    "    doc_id, anchors = page[0], page[1]     \n",
    "    return [(doc_id, anchor[1]) for anchor in anchors]\n",
    "\n",
    "\n",
    "def create_index(doc_pairs, directory, filter_tf=False):\n",
    "    inverted = InvertedIndex(base_dir=directory)\n",
    "    \n",
    "    doc_pairs = doc_pairs.map(lambda pair: (pair[0], tokenize(pair[1])))\n",
    "    doc_lengths = doc_pairs.mapValues(len).collectAsMap()\n",
    "    print(\"Done tokenization and doc lengths calculation\")\n",
    "    \n",
    "    # Calculate word counts and filter\n",
    "    word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1], inverted))\n",
    "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    w2df_dict = calculate_df(postings).collectAsMap()\n",
    "    \n",
    "    if filter_tf:\n",
    "        postings = postings.filter(lambda x: len(x[1]) > 50)\n",
    "        \n",
    "    print(\"Done posting lists creation and df creation\")\n",
    "\n",
    "    _ = partition_postings_and_write(postings, inverted).collect()\n",
    "    \n",
    "    # Collect all posting lists locations into one super-set\n",
    "    super_posting_locs = defaultdict(list)\n",
    "    for blob in client.list_blobs(BUCKET_NAME, prefix=directory):\n",
    "        if not blob.name.endswith(\"pickle\"):\n",
    "            continue\n",
    "        with blob.open(\"rb\") as f:\n",
    "            posting_locs = pickle.load(f)\n",
    "            for k, v in posting_locs.items():\n",
    "                super_posting_locs[k].extend(v)\n",
    "    \n",
    "    print(\"Done creating a posting locs list\")\n",
    "\n",
    "    \n",
    "    # Create and configure InvertedIndex instance\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "    print(\"Saved posting locs\")\n",
    "\n",
    "    inverted.df.update(w2df_dict)\n",
    "    print(\"Updated df\")\n",
    "\n",
    "    inverted.term_total.update(postings.flatMapValues(lambda x: x).map(lambda x: (x[0], x[1][1])).reduceByKey(add).collectAsMap())\n",
    "    print(\"Updated tf\")\n",
    "    \n",
    "    inverted.doc_lengths.update(doc_lengths)\n",
    "    print(\"Updated doc_lengths\")\n",
    "\n",
    "    inverted.avg_doc_length = np.mean(np.array(list(doc_lengths.values())))\n",
    "    print(\"Calculated average doc length\")\n",
    "    \n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30561012",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/10 14:41:43 WARN YarnAllocator: Container from a bad node: container_1709977868329_0043_01_000006 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:42.577]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:42.577]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:42.578]Killed by external signal\n",
      ".\n",
      "24/03/10 14:41:43 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1709977868329_0043_01_000006 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:42.577]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:42.577]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:42.578]Killed by external signal\n",
      ".\n",
      "24/03/10 14:41:43 ERROR YarnScheduler: Lost executor 5 on cluster-84ed-w-1.c.ir-project-415515.internal: Container from a bad node: container_1709977868329_0043_01_000006 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:42.577]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:42.577]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:42.578]Killed by external signal\n",
      ".\n",
      "24/03/10 14:41:43 WARN TaskSetManager: Lost task 75.0 in stage 2.0 (TID 136) (cluster-84ed-w-1.c.ir-project-415515.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709977868329_0043_01_000006 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:42.577]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:42.577]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:42.578]Killed by external signal\n",
      ".\n",
      "24/03/10 14:41:43 WARN YarnAllocator: Container from a bad node: container_1709977868329_0043_01_000004 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:43.767]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:43.767]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:43.768]Killed by external signal\n",
      ".\n",
      "24/03/10 14:41:43 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1709977868329_0043_01_000004 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:43.767]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:43.767]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:43.768]Killed by external signal\n",
      ".\n",
      "24/03/10 14:41:43 ERROR YarnScheduler: Lost executor 3 on cluster-84ed-w-0.c.ir-project-415515.internal: Container from a bad node: container_1709977868329_0043_01_000004 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:43.767]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:43.767]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:43.768]Killed by external signal\n",
      ".\n",
      "24/03/10 14:41:43 WARN TaskSetManager: Lost task 71.0 in stage 2.0 (TID 132) (cluster-84ed-w-0.c.ir-project-415515.internal executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709977868329_0043_01_000004 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:41:43.767]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:41:43.767]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:41:43.768]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done tokenization and doc lengths calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/10 14:48:58 WARN YarnAllocator: Container from a bad node: container_1709977868329_0043_01_000002 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:48:57.856]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:48:57.857]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:48:57.857]Killed by external signal\n",
      ".\n",
      "24/03/10 14:48:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1709977868329_0043_01_000002 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:48:57.856]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:48:57.857]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:48:57.857]Killed by external signal\n",
      ".\n",
      "24/03/10 14:48:58 ERROR YarnScheduler: Lost executor 2 on cluster-84ed-w-1.c.ir-project-415515.internal: Container from a bad node: container_1709977868329_0043_01_000002 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:48:57.856]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:48:57.857]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:48:57.857]Killed by external signal\n",
      ".\n",
      "24/03/10 14:48:58 WARN TaskSetManager: Lost task 40.0 in stage 5.0 (TID 351) (cluster-84ed-w-1.c.ir-project-415515.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709977868329_0043_01_000002 on host: cluster-84ed-w-1.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:48:57.856]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:48:57.857]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:48:57.857]Killed by external signal\n",
      ".\n",
      "24/03/10 14:53:25 WARN YarnAllocator: Container from a bad node: container_1709977868329_0043_01_000005 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:53:25.119]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:53:25.120]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:53:25.120]Killed by external signal\n",
      ".\n",
      "24/03/10 14:53:25 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1709977868329_0043_01_000005 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:53:25.119]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:53:25.120]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:53:25.120]Killed by external signal\n",
      ".\n",
      "24/03/10 14:53:25 ERROR YarnScheduler: Lost executor 4 on cluster-84ed-w-0.c.ir-project-415515.internal: Container from a bad node: container_1709977868329_0043_01_000005 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:53:25.119]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:53:25.120]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:53:25.120]Killed by external signal\n",
      ".\n",
      "24/03/10 14:53:25 WARN TaskSetManager: Lost task 114.0 in stage 5.0 (TID 426) (cluster-84ed-w-0.c.ir-project-415515.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709977868329_0043_01_000005 on host: cluster-84ed-w-0.c.ir-project-415515.internal. Exit status: 143. Diagnostics: [2024-03-10 14:53:25.119]Container killed on request. Exit code is 143\n",
      "[2024-03-10 14:53:25.120]Container exited with a non-zero exit code 143. \n",
      "[2024-03-10 14:53:25.120]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done posting lists creation and df creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating a posting locs list\n",
      "Saved posting locs\n",
      "Updated df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tf\n",
      "Updated doc_lengths\n",
      "Calculated average doc length\n",
      "Created anchor index\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "# Set up Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# Get list of blobs in bucket\n",
    "full_path = f\"gs://{BUCKET_NAME}/\"\n",
    "\n",
    "blobs = [b for b in client.list_blobs(BUCKET_NAME, prefix='wiki_files/') if b.name not in ['wiki_files/graphframes.sh', 'wiki_files/']]\n",
    "paths = [full_path + b.name for b in blobs]\n",
    "\n",
    "# Read parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "doc_anchor_pairs = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
    "doc_anchor_pairs = doc_anchor_pairs.flatMap(create_anchor_list).groupByKey().mapValues(list).map(lambda x: (x[0], \" \".join(x[1])))\n",
    "inverted_anchor = create_index(doc_anchor_pairs, f'indices/anchor_index/postings_anchor_gcp/', True)\n",
    "print('Created anchor index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176b4e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_token_counts_anchor = doc_anchor_pairs.map(lambda x: (x[0], len(tokenize(x[1]))))\n",
    "inverted_anchor.doc_lengths = doc_token_counts_anchor.collectAsMap()\n",
    "inverted_anchor.num_docs = len(inverted_anchor.doc_lengths.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38830072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://anchor_index.pkl [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 82.6 MiB/ 82.6 MiB]                                                \n",
      "Operation completed over 1 objects/82.6 MiB.                                     \n",
      "Anchor index saved to bucket successfully\n"
     ]
    }
   ],
   "source": [
    "# Write global stats and upload to Google Storage\n",
    "inverted_anchor.write_index('.', 'anchor_index')\n",
    "index_src = \"anchor_index.pkl\"\n",
    "index_dst = f'gs://{BUCKET_NAME}/indices/anchor_index/postings_anchor_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n",
    "print('Anchor index saved to bucket successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
