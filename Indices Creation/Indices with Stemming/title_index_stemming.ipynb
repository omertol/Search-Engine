{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7479625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "cluster-e10b  GCE       4                                             RUNNING  us-central1-a\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332981e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import hashlib\n",
    "from time import time\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from operator import add\n",
    "\n",
    "from google.cloud import storage\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb69ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 12:33:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InvertedIndex\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb05ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "# Set up stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8135b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROJECT_ID = 'irproject-416709'\n",
    "bucket_name = 'irproj_2605'\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdfe2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [porter.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
    "\n",
    "def word_count(id, tokens):\n",
    "    token_counts = Counter(tokens)\n",
    "    result = [(token, (id, count)) for token, count in token_counts.items()]\n",
    "    return result\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "def calculate_df(postings):\n",
    "    return postings.map(lambda x: (x[0], len(x[1]))) # (token, df) - df - in how many documents the term appeard\n",
    "\n",
    "def partition_postings_and_write(postings, index):\n",
    "    map_to_buckets = postings.map(lambda item: (token2bucket_id(item[0]), item)).groupByKey()\n",
    "    return map_to_buckets.map(lambda x: InvertedIndex.write_a_posting_list(x, index.base_dir, bucket_name))\n",
    "\n",
    "def create_anchor_list(page):\n",
    "    doc_id, anchors = page[0], page[1]     \n",
    "    return [(doc_id, anchor[1]) for anchor in anchors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef17ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 12:33:46 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "# Set up Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# Get list of blobs in bucket\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "\n",
    "blobs = [b for b in client.list_blobs(bucket_name, prefix='wiki_dump/') if b.name not in ['wiki_dump/graphframes.sh', 'wiki_dump/']]\n",
    "paths = [full_path + b.name for b in blobs]\n",
    "\n",
    "# Read parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# Count number of wiki pages\n",
    "N_docs = parquetFile.count()\n",
    "\n",
    "doc_text_pairs = parquetFile.select(\"id\", \"title\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = InvertedIndex(base_dir=f'indices/title_index/postings_title_gcp/')\n",
    "inverted.num_docs = N_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f93c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pairs = doc_text_pairs.map(lambda pair: (pair[0], tokenize(pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2df = calculate_df(postings)\n",
    "w2df_dict = w2df.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = partition_postings_and_write(postings, inverted).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96389d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75901688",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted.term_total.update(postings.flatMapValues(lambda x: x).map(lambda x: (x[0], x[1][1])).reduceByKey(add).collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f667acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted.doc_lengths = doc_pairs.map(lambda x: (x[0], len(x[1]))).collectAsMap()\n",
    "inverted.num_docs = len(inverted.doc_lengths.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted.avg_doc_length = np.mean(np.array(list(inverted.doc_lengths.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b177a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the global stats out\n",
    "inverted.write_index('.', 'index_title')\n",
    "# upload to gs\n",
    "index_src = \"index_title.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
