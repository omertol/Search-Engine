{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7479625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "cluster-e10b  GCE       4                                             RUNNING  us-central1-a\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332981e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import hashlib\n",
    "from time import time\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from operator import add\n",
    "\n",
    "from google.cloud import storage\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb69ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 12:40:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InvertedIndex\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb05ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "# Set up stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8135b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROJECT_ID = 'irproject-416709'\n",
    "bucket_name = 'irproj_2605'\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdfe2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [porter.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
    "\n",
    "def word_count(id, tokens):\n",
    "    token_counts = Counter(tokens)\n",
    "    result = [(token, (id, count)) for token, count in token_counts.items()]\n",
    "    return result\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "def calculate_df(postings):\n",
    "    return postings.map(lambda x: (x[0], len(x[1]))) # (token, df) - df - in how many documents the term appeard\n",
    "\n",
    "def partition_postings_and_write(postings, index):\n",
    "    map_to_buckets = postings.map(lambda item: (token2bucket_id(item[0]), item)).groupByKey()\n",
    "    return map_to_buckets.map(lambda x: InvertedIndex.write_a_posting_list(x, index.base_dir, bucket_name))\n",
    "\n",
    "def create_anchor_list(page):\n",
    "    doc_id, anchors = page[0], page[1]     \n",
    "    return [(doc_id, anchor[1]) for anchor in anchors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef17ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "# Set up Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# Get list of blobs in bucket\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "\n",
    "blobs = [b for b in client.list_blobs(bucket_name, prefix='wiki_dump/') if b.name not in ['wiki_dump/graphframes.sh', 'wiki_dump/']]\n",
    "paths = [full_path + b.name for b in blobs]\n",
    "\n",
    "# Read parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# Count number of wiki pages\n",
    "N_docs = parquetFile.count()\n",
    "\n",
    "doc_text_pairs = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
    "doc_text_pairs = doc_text_pairs.flatMap(create_anchor_list).groupByKey().mapValues(list).map(lambda x: (x[0], \" \".join(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f9fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = InvertedIndex(base_dir=f'indices/anchor_index/postings_anchor_gcp/')\n",
    "inverted.num_docs = N_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f93c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pairs = doc_text_pairs.map(lambda pair: (pair[0], tokenize(pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da2ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "postings_filtered = postings.filter(lambda x: len(x[1])>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20fe102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 12:43:58 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000004 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:43:58.206]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:43:58.206]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:43:58.206]Killed by external signal\n",
      ".\n",
      "24/03/11 12:43:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1710065956596_0051_01_000004 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:43:58.206]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:43:58.206]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:43:58.206]Killed by external signal\n",
      ".\n",
      "24/03/11 12:43:58 ERROR YarnScheduler: Lost executor 4 on cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000004 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:43:58.206]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:43:58.206]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:43:58.206]Killed by external signal\n",
      ".\n",
      "24/03/11 12:43:58 WARN TaskSetManager: Lost task 30.0 in stage 5.0 (TID 216) (cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000004 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:43:58.206]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:43:58.206]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:43:58.206]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:55 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000003 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:55.401]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:55.401]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:55.409]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:55 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1710065956596_0051_01_000003 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:55.401]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:55.401]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:55.409]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:55 ERROR YarnScheduler: Lost executor 3 on cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000003 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:55.401]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:55.401]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:55.409]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:55 WARN TaskSetManager: Lost task 67.0 in stage 5.0 (TID 254) (cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000003 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:55.401]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:55.401]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:55.409]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:58 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000005 on host: cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:58.236]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:58.236]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:58.236]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1710065956596_0051_01_000005 on host: cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:58.236]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:58.236]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:58.236]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:58 ERROR YarnScheduler: Lost executor 5 on cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000005 on host: cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:58.236]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:58.236]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:58.236]Killed by external signal\n",
      ".\n",
      "24/03/11 12:46:58 WARN TaskSetManager: Lost task 57.0 in stage 5.0 (TID 244) (cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000005 on host: cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:46:58.236]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:46:58.236]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:46:58.236]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:16 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000001 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:16.160]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:16.160]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:16.161]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:16 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1710065956596_0051_01_000001 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:16.160]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:16.160]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:16.161]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:16 ERROR YarnScheduler: Lost executor 1 on cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000001 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:16.160]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:16.160]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:16.161]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:16 WARN TaskSetManager: Lost task 75.0 in stage 5.0 (TID 262) (cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000001 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:16.160]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:16.160]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:16.161]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:32 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000002 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:31.857]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:31.858]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:31.858]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:32 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1710065956596_0051_01_000002 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:31.857]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:31.858]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:31.858]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:32 ERROR YarnScheduler: Lost executor 2 on cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000002 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:31.857]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:31.858]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:31.858]Killed by external signal\n",
      ".\n",
      "24/03/11 12:47:32 WARN TaskSetManager: Lost task 79.0 in stage 5.0 (TID 266) (cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000002 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:47:31.857]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:47:31.858]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:47:31.858]Killed by external signal\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/11 12:48:29 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000006 on host: cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:48:29.522]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:48:29.522]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:48:29.522]Killed by external signal\n",
      ".\n",
      "24/03/11 12:48:29 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1710065956596_0051_01_000006 on host: cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:48:29.522]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:48:29.522]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:48:29.522]Killed by external signal\n",
      ".\n",
      "24/03/11 12:48:29 ERROR YarnScheduler: Lost executor 6 on cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000006 on host: cluster-e10b-w-0.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 12:48:29.522]Container killed on request. Exit code is 143\n",
      "[2024-03-11 12:48:29.522]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 12:48:29.522]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:15 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000012 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:15.336]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:15.336]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:15.337]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 12 for reason Container from a bad node: container_1710065956596_0051_01_000012 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:15.336]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:15.336]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:15.337]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:15 ERROR YarnScheduler: Lost executor 12 on cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000012 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:15.336]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:15.336]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:15.337]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:15 WARN TaskSetManager: Lost task 101.0 in stage 6.0 (TID 416) (cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000012 on host: cluster-e10b-w-1.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:15.336]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:15.336]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:15.337]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:35 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000013 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:35.307]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:35.307]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:35.311]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:35 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 13 for reason Container from a bad node: container_1710065956596_0051_01_000013 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:35.307]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:35.307]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:35.311]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:35 ERROR YarnScheduler: Lost executor 13 on cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000013 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:35.307]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:35.307]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:35.311]Killed by external signal\n",
      ".\n",
      "24/03/11 13:12:35 WARN TaskSetManager: Lost task 98.0 in stage 6.0 (TID 413) (cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000013 on host: cluster-e10b-w-3.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:12:35.307]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:12:35.307]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:12:35.311]Killed by external signal\n",
      ".\n",
      "24/03/11 13:14:28 WARN YarnAllocator: Container from a bad node: container_1710065956596_0051_01_000007 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:14:28.808]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:14:28.808]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:14:28.809]Killed by external signal\n",
      ".\n",
      "24/03/11 13:14:28 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container from a bad node: container_1710065956596_0051_01_000007 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:14:28.808]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:14:28.808]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:14:28.809]Killed by external signal\n",
      ".\n",
      "24/03/11 13:14:28 ERROR YarnScheduler: Lost executor 7 on cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal: Container from a bad node: container_1710065956596_0051_01_000007 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:14:28.808]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:14:28.808]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:14:28.809]Killed by external signal\n",
      ".\n",
      "24/03/11 13:14:28 WARN TaskSetManager: Lost task 104.0 in stage 6.0 (TID 421) (cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710065956596_0051_01_000007 on host: cluster-e10b-w-2.us-central1-a.c.irproject-416709.internal. Exit status: 143. Diagnostics: [2024-03-11 13:14:28.808]Container killed on request. Exit code is 143\n",
      "[2024-03-11 13:14:28.808]Container exited with a non-zero exit code 143. \n",
      "[2024-03-11 13:14:28.809]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b818e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = partition_postings_and_write(postings_filtered, inverted).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c96389d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect all posting lists locations into one super-set\n",
    "# super_posting_locs = defaultdict(list)\n",
    "# for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "#     if not blob.name.endswith(\"pickle\"):\n",
    "#         continue\n",
    "#     with blob.open(\"rb\") as f:\n",
    "#         posting_locs = pickle.load(f)\n",
    "#         for k, v in posting_locs.items():\n",
    "#             super_posting_locs[k].extend(v)\n",
    "\n",
    "# Initialize Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "bucket_name = 'irproj_2605'\n",
    "prefix = 'indices/anchor_index/postings_anchor_gcp'\n",
    "\n",
    "# Collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name, prefix=prefix):\n",
    "    if blob.name.endswith(\"pickle\"):  # Assuming posting lists locations are stored in pickle files\n",
    "        with blob.open(\"rb\") as f:\n",
    "            posting_locs = pickle.load(f)\n",
    "            for k, v in posting_locs.items():\n",
    "                super_posting_locs[k].extend(v)\n",
    "\n",
    "# Example: Updating the inverted index with the aggregated posting locations\n",
    "# Assuming `inverted` is your InvertedIndex object instance\n",
    "inverted.posting_locs = super_posting_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "258a232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the posting locations dictionary to the inverted index\n",
    "# inverted.posting_locs = super_posting_locs\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75901688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inverted.term_total.update(postings.flatMapValues(lambda x: x).map(lambda x: (x[0], x[1][1])).reduceByKey(add).collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f667acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inverted.doc_lengths = doc_pairs.map(lambda x: (x[0], len(x[1]))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d569baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted.avg_doc_length = np.mean(np.array(list(inverted.doc_lengths.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27b177a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://index_anchor.pkl [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 78.4 MiB/ 78.4 MiB]                                                \n",
      "Operation completed over 1 objects/78.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# write the global stats out\n",
    "inverted.write_index('.', 'index_anchor')\n",
    "# upload to gs\n",
    "index_src = \"index_anchor.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
